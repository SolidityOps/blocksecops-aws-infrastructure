apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-info
  namespace: kube-system
data:
  cluster.name: "solidity-security-eks-production"
  cluster.version: "1.28"
  cluster.region: "us-west-2"
  cluster.endpoint.access: "private-public"
  cluster.logging.enabled: "false"
  cluster.environment: "production"
  cluster.node.instance.types: "t3.large,t3.xlarge,m5.large"
  cluster.autoscaling.enabled: "true"
  cluster.high.availability: "true"
  cluster.multi.az: "true"
  cluster.backup.enabled: "true"
  cluster.monitoring.enhanced: "true"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: cluster-autoscaler
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::ACCOUNT_ID:role/solidity-security-eks-production-cluster-autoscaler-role"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: cluster-autoscaler
spec:
  replicas: 2  # High availability for production
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - cluster-autoscaler
              topologyKey: kubernetes.io/hostname
      containers:
        - name: cluster-autoscaler
          image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
          command:
            - ./cluster-autoscaler
            - --v=2  # Less verbose for production
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=priority
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/solidity-security-eks-production
            - --balance-similar-node-groups
            - --scale-down-enabled=true
            - --scale-down-delay-after-add=15m  # More conservative for production
            - --scale-down-unneeded-time=15m
            - --scale-down-delay-after-delete=30s
            - --scale-down-delay-after-failure=3m
            - --scale-down-utilization-threshold=0.6  # More conservative threshold
            - --skip-nodes-with-system-pods=false
            - --max-node-provision-time=15m
          resources:
            limits:
              cpu: 200m
              memory: 600Mi
            requests:
              cpu: 200m
              memory: 600Mi